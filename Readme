## On the Computational Entanglement of Distant Features in Adversarial Machine Learning

Research paper link: https://arxiv.org/abs/2309.15669

In this research, we introduce "computational entanglement," an emerging phenomenon where overparameterized neural networks exploit noise patterns in a manner that correlates with relativistic effects such as time dilation and length contraction. Our findings demonstrate that feedforward linear networks with excessive parameters can achieve zero training loss by fitting random noise, even with unseen test samples. Additionally, we show a novel application of computational entanglement: transforming worst-case adversarial examples—highly noisy and uninterpretable to humans—into outputs that are recognizable to human observers. These results offer new insights into the role of non-robust features in adversarial example generation and highlight the significance of understanding computational entanglement in this
